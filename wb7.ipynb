{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Рабочая тетрадь 7\n",
    "#### Теоретический материал - Нейронные сети"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Обучение персептрона\n",
    "Персептрон представляет собой элементарную часть нейронной сети. Одиночный персептрон является линейным бинарным классификатором. В этой лекции мы рассмотрим процедуру обучения персептрона для классификации данных. Поскольку персептрон представляет собой бинарный классификатор, то мы будем рассматривать лишь два класса.\n",
    "Пусть мы рассматриваем некоторое множество (конечное или бесконечное) n-мерных векторов, которые будем обозначать x=(x_1,x_2,...,x_n)\n",
    "Будем считать, что это множество разбивается на два класса, которые мы будем обозначать +1 и -1. Поэтому возникает задача построения функции, которая задана на нашем множестве векторов, и принимает значения в множестве {+1,-1}. В качестве такой функции может выступать персептрон. С алгебраической точки зрения персептрон состоит из вектора весов w=(w_0,w_1,w_2,...,w_n).\n",
    "При этом персептрон работает по формуле\n",
    "y=sign(w_0  + x_1 w_1  + x_2 w_2  + ...+ x_n w_n),\n",
    "где функция sign(t) равна +1, если t ≥ 0, и равна -1, если t < 0.\n",
    "Приведем алгоритм обучения персептрона. Пусть у нас есть набор обучающих данных {(x,d)}, где x - это различные вектора, а d из множества {+1,-1} указывает к какому классу относится наш вектор.\n",
    "<ol>\n",
    "    <li>Положим вектор весов w равным нулю.</li>\n",
    "    <li>Повторять N раз следующие шаги:</li>\n",
    "    <li>Для каждого тестового набора (x,d):</li>\n",
    "    <li>Вычислить y = sing[(x,w)].</li>\n",
    "    <li>Если yd < 0, то скорректировать веса w_0  = w_0  + ad,w_i  = w_i  + adx_i,i = 1,2,...,n.</li></ol>\n",
    "Описанный алгоритм довольно легко программировать.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.1.1 Пример\n",
    "Рассмотрим программу обучения персептрона на языке Python. Сначала рассмотрим основной класс персептрона, который умеет учиться по тестовым данным:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# класс, который реализует персептрон и его обучение\n",
    "class Perceptron:\n",
    "    def __init__(self, N):\n",
    "        self.w = list()\n",
    "        for i in range(N):\n",
    "            self.w.append(0)\n",
    "\n",
    "    # метод для вычисления значения персептрона\n",
    "    def calc(self, x):\n",
    "        res = 0\n",
    "        for i in range(len(self.w)):\n",
    "            res = res + self.w[i] * x[i]\n",
    "        return res\n",
    "\n",
    "    # пороговая функция активации персептрона\n",
    "    def sign(self, x):\n",
    "        if self.calc(x) > 0:\n",
    "            return 1\n",
    "        return -1\n",
    "\n",
    "    # обучение на одном примере\n",
    "    def learn(self, la, x, y):\n",
    "        # обучаем только, когда результат неверный\n",
    "        if y * self.calc(x) <= 0:\n",
    "            for i in range(len(self.w)):\n",
    "                self.w[i] = self.w[i] + la * x[i] * y\n",
    "\n",
    "    # обучение по всем данным Т - кортеж примеров\n",
    "    def learning(self, la, T):\n",
    "        # цикл обучения\n",
    "        for n in range(100):\n",
    "            # обучение по всему набору примеров\n",
    "            for t in T:\n",
    "                self.learn(la, t[0], t[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "В строке 25 мы осуществляем корректировку весов. Посмотрим, как учится и работает наш персептрон."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# создаем класс двумерного персептрона\n",
    "perceptron = Perceptron(2)\n",
    "la = .1  # константа обучения\n",
    "# создаем примеры\n",
    "T = list()\n",
    "T.append([[2, 1], 1])\n",
    "T.append([[3, 2], 1])\n",
    "T.append([[4, 1], 1])\n",
    "T.append([[1, 2], -1])\n",
    "T.append([[2, 3], -1])\n",
    "T.append([[5, 7], -1])\n",
    "perceptron.learning(la, T)  # обучение персептрона\n",
    "print(perceptron.w)  # печатаем веса\n",
    "# проверим работу на тестовых примерах\n",
    "print(perceptron.sign([1.5, 2]))\n",
    "print(perceptron.sign([3, 1.5]))\n",
    "print(perceptron.sign([5, 1]))\n",
    "print(perceptron.sign([5, 10]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Видим, что что наш персептрон отлично научился распознавать образы, относя к классу 1 те вектора, у которых первая компонента больше второй, и к классу -1 в противном случае. Хотя устройство персептронов довольно простое эти конструкции могут решать и практические задачи. Кроме того, из таких персептронов состоят нейронные сети."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Реализация нейронной сети на Python\n",
    "Нейронная сеть — это функциональная единица машинного или глубокого обучения. Она имитирует поведение человеческого мозга, поскольку основана на концепции биологических нейронных сетей.\n",
    "Наиболее распространенный тип нейронной сети, называемый многослойным персептроном (MLP), представляет собой функцию, которая отображает входные данные в выходные данные. MLP имеет один входной слой и один выходной слой. Между ними может быть один или несколько скрытых слоев. Входной слой имеет тот же набор нейронов, что и признаки. Скрытые слои также могут иметь более одного нейрона. Каждый нейрон представляет собой линейную функцию, к которой применяется функция активации для решения сложных задач. Выход каждого слоя подается в качестве входных данных для всех нейронов следующих слоев.\n",
    "Нейронные сети способны решать множество задач. В основном они состоят из таких компонентов:\n",
    "<ul>\n",
    "    <li>входной слой (получение и передача данных);</li>\n",
    "    <li>скрытый слой (вычисление);</li>\n",
    "\t<li>выходной слой. Чтобы реализовать нейросеть, необходимо понимать, как ведут себя нейроны. Нейрон одновременно принимает несколько входов, обрабатывает эти данные и выдает один выход. Нейронная сеть представляет собой блоки ввода и вывода, где каждое соединение имеет соответствующие веса (это сила связи нейронов; чем вес больше, тем один нейрон сильнее влияет на другой). Данные всех входов умножаются на веса:</li>\n",
    "    <li>x → x*w_1;</li>\n",
    "    <li>y → y*w_2.</li>\n",
    "\n",
    "Входы после взвешивания суммируются с прибавлением значения порога «c»:\n",
    "    <center>xw_1  + yw_2  + c</center>\n",
    "\n",
    "Полученное значение пропускается через функцию активации (сигмоиду), которая преобразует входы в один выход:\n",
    "\n",
    "    z = f(xw_1  + yw_2  + c)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Так выглядит сигмоида:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "    \n",
    "Интервал результатов сигмоиды — от 0 до 1. Отрицательные числа стремятся к нулю, а положительные — к единице.\n",
    "Например. Пусть нейрон имеет следующие значения: w = [0,1] c = 4.\n",
    "Входной слой: x = 2,y = 3.\t\n",
    "\n",
    "((xw_1) + (yw_2)) + c = 20 + 31 + 4 = 7.\n",
    "z = f(7) = 0.99.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.1.2 Пример\n",
    "Для написания кода нейрона будем использовать библиотеку Python NumPy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Функция активации: f(x) = 1 / (1 + e^(-x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def feedforward(self, inputs):\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return sigmoid(total)\n",
    "\n",
    "\n",
    "weights = np.array([0, 1])  # w1 = 0, w2 = 1\n",
    "bias = 4  # c = 4\n",
    "n = Neuron(weights, bias)\n",
    "x = np.array([2, 3])  # x = 2, y = 3\n",
    "print(n.feedforward(x))  # 0.9990889488055994"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Нейросеть состоит из множества соединенных между собой нейронов.<br>\n",
    "Пример несложной нейронной сети\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "где:\n",
    "<ul>\n",
    "    <li>x_1,x_2 — входной слой;</li>\n",
    "    <li>h_1,h_2 — скрытый слой с двумя нейронами;</li>\n",
    "    <li>o_1 — выходной слой.</li>\n",
    "</ul>\n",
    "Например, представим, что нейроны из графика выше имеют веса `[0,1]`. Пороговое значение (b) у обоих нейронов равно 0 и они имеют идентичную сигмоиду.<br>\n",
    "При входных данных x = `[2, 3]` получим:<br>\n",
    "h_1  = h_2  = f(wx+b) = f((02) + (1*3) +0) = f(3) = 0.95.<br>\n",
    "o_1  = f(w*`[h_1,h_2]`+b) = f((0h_1) + (1h_2) +0) = f(0.95) = 0.72.<br>\n",
    "Входные данные по нейронам передаются до тех пор, пока не получатся выходные значения."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class OurNeuralNetwork:\n",
    "    '''\n",
    "    Данные нейросети:\n",
    "        - два входа\n",
    "        - два нейрона в скрытых слоях (h1, h2)\n",
    "        - выход (о1)\n",
    "    Нейроны имеют идентичные веса и пороги:\n",
    "        - w = [0, 1]\n",
    "        - b = 0\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        weights = np.array([0, 1])\n",
    "        bias = 0\n",
    "        # Класс Neuron из предыдущего раздела\n",
    "        self.h1 = Neuron(weights, bias)\n",
    "        self.h2 = Neuron(weights, bias)\n",
    "        self.o1 = Neuron(weights, bias)\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        # Входы для о1 - это выходы h1 и h2\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "        return out_o1\n",
    "\n",
    "\n",
    "network = OurNeuralNetwork()\n",
    "x = np.array([2, 3])\n",
    "print(network.feedforward(x))  # 0.7216325609518421"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Обучение нейронной сети\n",
    "<p>Обучение нейросети — это подбор весов, которые соответствуют всем входам для решения поставленных задач.</p>\n",
    "<p>Класс нейронной сети:</p>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.input = x\n",
    "        self.weights1 = np.random.rand(self.input.shape[1], 4)\n",
    "        self.weights2 = np.random.rand(4, 1)\n",
    "        self.y = y\n",
    "        self.output = np.zeros(y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>Каждый этап процесса обучения состоит из:\n",
    "<ul>\n",
    "    <li>прямого распространения (прогнозируемый выход);</li>\n",
    "    <li>обратного распространения (обновление весов и смещений).</li>\n",
    "</ul></p>\n",
    "<p>Например, дана двуслойная нейросеть:</p>\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "<p>В данном случае на выход ŷ влияют только две переменные — w (веса) и b (смещение). Настройку весов и смещений из данных входа или процесс обучения нейросети можно изобразить так:</p>\n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "<p><b>Прямое распространение</b></p>\n",
    "<p>Как видно, формула прямого распространения представляет собой несложное вычисление:</p>\n",
    "\n",
    "![image-3.png](attachment:image-3.png)\n",
    "\n",
    "<p>Далее необходимо добавить в код функцию прямого распространения. Предположим, что смещения в этом случае будут равны 0.</p>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.input = x\n",
    "        self.weights1 = np.random.rand(self.input.shape[1], 4)\n",
    "        self.weights2 = np.random.rand(4, 1)\n",
    "        self.y = y\n",
    "        self.output = np.zeros(y.shape)\n",
    "\n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>Чтобы вычислить ошибку прогноза, необходимо использовать функцию потери. В примере уместно воспользоваться формулой суммы квадратов ошибок — средним значением между прогнозируемым и фактическим результатами:</p>\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "<p><b>Обратное распространение</b></p>\n",
    "<p>Обратное распространение позволяет измерить производные в обратном порядке — от конца к началу, и скорректировать веса и смещения. Для этого необходимо узнать производную функции потери — тангенс угла наклона.</p>\n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "<p>Производная функции по отношению к весам и смещениям позволяет узнать градиентный спуск. Производная функции потери не содержит весов и смещений, для ее вычисления необходимо добавить правило цепи:</p>\n",
    "\n",
    "![image-3.png](attachment:image-3.png)\n",
    "\n",
    "<p>Благодаря этому правилу можно регулировать веса. Добавляем в код Python функцию обратного распространения:</p>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.input = x\n",
    "        self.weights1 = np.random.rand(self.input.shape[1], 4)\n",
    "        self.weights2 = np.random.rand(4, 1)\n",
    "        self.y = y\n",
    "        self.output = np.zeros(y.shape)\n",
    "\n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n",
    "\n",
    "    def backprop(self):\n",
    "        # применение правила цепи для нахождения производной функции потерь по весу2 и весу1\n",
    "        d_weights2 = np.dot(self.layer1.T, (2 * (self.y - self.output) * sigmoid_derivative(self.output)))\n",
    "        d_weights1 = np.dot(self.input.T, (np.dot(2 * (self.y - self.output) * sogmoid_derivative(self.output),\n",
    "                                                  self.weights2.T) * sigmoid_derivative(self.layer1)))\n",
    "\n",
    "        # обновление веса производной (наклона) функции потерь\n",
    "        self.weigths1 += d_weights1\n",
    "        self.weights2 += d_weights2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>Нейронные сети базируются на определенных алгоритмах и математических функциях. Сначала может казаться, что разобраться в них довольно сложно. Но существуют готовые библиотеки машинного обучения для построения и тренировки нейросетей, позволяющие не углубляться в их устройство.</p>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Задание"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def feedforward(self, inputs):\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return sigmoid(total)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    '''\n",
    "    Данные нейросети:\n",
    "        - три входа (x1, x2, x3)\n",
    "        - три нейрона в скрытых слоях (h1, h2, h3)\n",
    "        - выход (о1)\n",
    "    Нейроны имеют идентичные веса и пороги:\n",
    "        - w = [0.5, 0.5, 0.5]\n",
    "        - b = 0\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        weights = np.array([.5, .5, .5])\n",
    "        bias = 0\n",
    "\n",
    "        self.h1 = Neuron(weights, bias)\n",
    "        self.h2 = Neuron(weights, bias)\n",
    "        self.h3 = Neuron(weights, bias)\n",
    "        self.o1 = Neuron(weights, bias)\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        out_h3 = self.h3.feedforward(x)\n",
    "\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2, out_h3]))\n",
    "        return out_o1\n",
    "\n",
    "\n",
    "network = NeuralNetwork()\n",
    "x = np.array([2, 3, 4])\n",
    "print(network.feedforward(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def feedforward(self, inputs):\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return sigmoid(total)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    '''\n",
    "    Данные нейросети:\n",
    "        - два входа (x1, x2)\n",
    "        - два нейрона в скрытых слоях (h1, h2)\n",
    "        - два выхода (о1, o2)\n",
    "    Нейроны имеют идентичные веса и пороги:\n",
    "        - w = [1, 0]\n",
    "        - b = 1\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        weights = np.array([1, 0])\n",
    "        bias = 1\n",
    "\n",
    "        self.h1 = Neuron(weights, bias)\n",
    "        self.h2 = Neuron(weights, bias)\n",
    "\n",
    "        self.o1 = Neuron(weights, bias)\n",
    "        self.o2 = Neuron(weights, bias)\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "        out_o2 = self.o2.feedforward(np.array([out_h1, out_h2]))\n",
    "        return out_o1, out_o2\n",
    "\n",
    "\n",
    "network = NeuralNetwork()\n",
    "x = np.array([2, 3])\n",
    "print(*network.feedforward(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Задание\n",
    "Реализуйте классы нейронных сетей с использованием других функций активации.\n",
    "\n",
    "![image.png](attachment:image.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def feedforward_sigmoid(self, inputs):\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return sigmoid(total)\n",
    "\n",
    "    def feedforward_tanh(self, inputs):\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return tanh(total)\n",
    "\n",
    "    def feedforward_relu(self, inputs):\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return relu(total)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    '''\n",
    "    Данные нейросети:\n",
    "        - три входа (x1, x2, x3)\n",
    "        - три нейрона в скрытых слоях (h1, h2, h3)\n",
    "        - выход (о1)\n",
    "    Нейроны имеют идентичные веса и пороги:\n",
    "        - w = [0.5, 0.5, 0.5]\n",
    "        - b = 0\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        weights = np.array([.5, .5, .5])\n",
    "        bias = 0\n",
    "\n",
    "        self.h1 = Neuron(weights, bias)\n",
    "        self.h2 = Neuron(weights, bias)\n",
    "        self.h3 = Neuron(weights, bias)\n",
    "\n",
    "        self.o1 = Neuron(weights, bias)\n",
    "\n",
    "    def feedforward_sigmoid(self, x):\n",
    "        out_h1 = self.h1.feedforward_sigmoid(x)\n",
    "        out_h2 = self.h2.feedforward_sigmoid(x)\n",
    "        out_h3 = self.h3.feedforward_sigmoid(x)\n",
    "\n",
    "        out_o1 = self.o1.feedforward_sigmoid(np.array([out_h1, out_h2, out_h3]))\n",
    "        return out_o1\n",
    "\n",
    "    def feedforward_tanh(self, x):\n",
    "        out_h1 = self.h1.feedforward_tanh(x)\n",
    "        out_h2 = self.h2.feedforward_tanh(x)\n",
    "        out_h3 = self.h3.feedforward_tanh(x)\n",
    "\n",
    "        out_o1 = self.o1.feedforward_tanh(np.array([out_h1, out_h2, out_h3]))\n",
    "        return out_o1\n",
    "\n",
    "    def feedforward_relu(self, x):\n",
    "        out_h1 = self.h1.feedforward_relu(x)\n",
    "        out_h2 = self.h2.feedforward_relu(x)\n",
    "        out_h3 = self.h3.feedforward_relu(x)\n",
    "\n",
    "        out_o1 = self.o1.feedforward_relu(np.array([out_h1, out_h2, out_h3]))\n",
    "        return out_o1\n",
    "\n",
    "\n",
    "network = NeuralNetwork()\n",
    "x = np.array([2, 3, 4])\n",
    "print(\n",
    "    f'Sigmoid: {network.feedforward_sigmoid(x)}\\nTanh: {network.feedforward_tanh(x)}\\nReLU: {network.feedforward_relu(x)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Введение в нейронные сети с помощью Scikit-Learn в Python\n",
    "<p>Теперь мы знаем, что такое нейронные сети и какие шаги необходимо выполнить, чтобы построить простую нейронную сеть с плотными связями. В этом разделе мы попытаемся построить простую нейронную сеть, которая предсказывает класс, к которому принадлежит данное растение ириса. Мы будем использовать библиотеку Python Scikit-Learn для создания нашей нейронной сети.</p>\n",
    "<p>Sklearn предоставляет 2 оценщика для задач классификации и регрессии соответственно:</p>\n",
    "<ul>\n",
    "    <li>MLPClassifier;</li>\n",
    "    <li>MLPRegressor</li>\n",
    "</ul>\n",
    "<p>Начнем с импорта необходимых библиотек.</p>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Загрузка библиотек\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MLPClassifier\n",
    "<p>Загрузка данных</p>\n",
    "<p>Мы будем загружать два набора данных.</p>\n",
    "<p>Набор данных цифр: мы будем использовать набор данных цифр, который имеет изображения размером 8x8 для цифр 0-9. Ниже мы будем использовать цифровые данные для задач классификации.</p>\n",
    "<p>Набор данных о жилье в Бостоне: мы будем использовать набор данных о жилье в Бостоне, который содержит информацию о различных свойствах дома, таких как среднее количество комнат, уровень преступности на душу населения в городе и т. д. Мы будем использовать его для задач регрессии.</p>\n",
    "<p>Sklearn предоставляет оба этих набора данных. Мы можем загрузить их, вызвав методы load_digits() и load_boston().</p>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits, load_boston\n",
    "\n",
    "digits = load_digits()\n",
    "x_digits, y_digits = digits.data, digits.target\n",
    "print('Dataset Sizes : ', x_digits.shape, y_digits.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "x_boston, y_boston = boston.data, boston.target\n",
    "print('Dataset Sizes : ', x_boston.shape, y_boston.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><b>Классификация</b></p>\n",
    "<p>MLPClassifier — это клвсс, доступный как часть модуля neuro_network sklearn для выполнения задач классификации с использованием многослойного персептрона.</p>\n",
    "<p>Как обычно разделим набор данных на две части:</p>\n",
    "<ul>\n",
    "    <li>данные обучения, которые будут использоваться для модели обучения;</li>\n",
    "    <li>тестовые данные, по которым будет проверяться точность обученной модели.</li>\n",
    "</ul>\n",
    "<p>Функция train_test_split модуля model_selection sklearn поможет нам разделить данные на два набора: 80% для обучения и 20% для тестирования. Мы также используем seed(random_state=123) с train_test_split, чтобы мы всегда получали одно и то же разделение и могли сравнивать и воспроизволить результаты в будущем.</p>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_digits, y_digits, train_size=.80, test_size=.20,\n",
    "                                                    stratify=y_digits, random_state=123)\n",
    "print('Train/Test Sizes : ', x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для начала натренируем модель MLPClassifier с параметрами по умолчанию для тренировочных данных."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_classifier = MLPClassifier(random_state=123)\n",
    "mlp_classifier.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_preds = mlp_classifier.predict(x_test)\n",
    "\n",
    "print(y_preds[:15])\n",
    "print(y_test[:15])\n",
    "# Метод Score для оценки точности моделей классификации\n",
    "print('Test Accuracy : %.3f' % mlp_classifier.score(x_test, y_test))\n",
    "\n",
    "print('Training Accuracy : %.3f' % mlp_classifier.score(x_train, y_train))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cоздадим метод plot_confusion_matrix(), который принимает исходные и предсказанные метки данных по модели. Затем он строит матрицу путаницы, используя matplotlib."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_test, y_preds):\n",
    "    conf_mat = confusion_matrix(y_test, y_preds)\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    plt.matshow(conf_mat, cmap=plt.cm.Blues, fignum=1)\n",
    "    plt.yticks(range(10), range(10))\n",
    "    plt.xticks(range(10), range(10))\n",
    "    plt.colorbar()\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            plt.text(i - .2, j + .1, str(conf_mat[j, i]), color='tab:red')\n",
    "\n",
    "\n",
    "plot_confusion_matrix(y_test, mlp_classifier.predict(x_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>Ниже приведен список важных атрибутов, доступных с MLPClassifier, которые могут предоставить значимую информацию после обучения модели.</p>\n",
    "<ul>\n",
    "    <li>loss_ — возвращает убыток после завершения процесса обучения.</li>\n",
    "    <li>coefs_ — возвращает массив длины n_layers-1, где каждый элемент представляет веса, связанные с уровнем i.</li>\n",
    "    <li>intercepts_ — возвращает массив длины n_layers-1, где каждый элемент представляет собой перехват, связанный с персептронами слоя i.</li>\n",
    "    <li>n_iter_ — количество итераций, для которых выполнялась оценка.</li>\n",
    "    <li>out_activation_ — возвращает имя функции активации выходного слоя.</li>\n",
    "</ul>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Loss : ', mlp_classifier.loss_)\n",
    "print('Number of Coefs : ', len(mlp_classifier.coefs_))\n",
    "print('Number of Intercepts : ', len(mlp_classifier.intercepts_))\n",
    "print('Number of Iterations for Which Estimator Ran : ', mlp_classifier.n_iter_)\n",
    "print('Name of Output Layer Activation Function : ', mlp_classifier.out_activation_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MLPRegressor\n",
    "<p>MLPRegressor — это класс, доступный как часть библиотеки neuro_network sklearn для выполнения задач регрессии с использованием многослойного персептрона. Также разделим набор данных на две части:</p>\n",
    "<ul>\n",
    "    <li>данные обучения (80%), которые будут использоваться для модели обучения;</li>\n",
    "    <li>тестовые данные (20%), по которым будет проверяться точность обученной модели.</li>\n",
    "</ul>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_boston, y_boston, train_size=.80, test_size=.20, random_state=123)\n",
    "print('Train/Test Sizes : ', x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp_regressor = MLPRegressor(random_state=123)\n",
    "mlp_regressor.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_preds = mlp_regressor.predict(x_test)\n",
    "\n",
    "print(y_preds[:10])\n",
    "print(y_test[:10])\n",
    "# Метод Score оценивает точность моделей классификации\n",
    "print('Test R^2 Score : %.3f' % mlp_regressor.score(x_test, y_test))\n",
    "print('Training R^2 Score : %.3f' % mlp_regressor.score(x_train, y_train))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "MLPRegressor имеет все атрибуты такие же, как и у MLPClassifier:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Loss : ', mlp_regressor.loss_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Number of Coefs : ', len(mlp_regressor.coefs_))\n",
    "[weights.shape for weights in mlp_regressor.coefs_]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Number of Iterations for Which Estimator Ran : ', mlp_regressor.n_iter_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Name of Output Layer Activation Function : ', mlp_regressor.out_activation_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Задание\n",
    "Используйте классы MLPClassified и MLPRegressor для классификации и регрессии произвольных данных из интернета. Проведите анализ атрибуты, полученных моделей.\n",
    "\n",
    "Для классификации можете взять набор данных Ирисов:\n",
    "https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv\n",
    "а для регрессии датасет зависимости заработной платы от опыта работы:\n",
    "https://raw.githubusercontent.com/AnnaShestova/salary-years-simple-linear-regression/master/Salary_Data.csv \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "urls = {\n",
    "    'class': r'https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv',\n",
    "    'reg': r'https://raw.githubusercontent.com/AnnaShestova/salary-years-simple-linear-regression/master/Salary_Data.csv'\n",
    "}\n",
    "print('**********MLPClassifier**********\\n')\n",
    "class_data = pd.read_csv(urls['class'])\n",
    "x_data, y_data = class_data['sepal.length'], class_data['sepal.width']\n",
    "print('Dataset Sizes : ', x_data.shape, y_data.shape)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_digits, y_digits, train_size=.80, test_size=.20,\n",
    "                                                    stratify=y_digits, random_state=123)\n",
    "print('Train/Test Sizes : ', x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_classifier = MLPClassifier(random_state=123)\n",
    "mlp_classifier.fit(x_train, y_train)\n",
    "\n",
    "y_preds = mlp_classifier.predict(x_test)\n",
    "\n",
    "print(y_preds[:15])\n",
    "print(y_test[:15])\n",
    "# Метод Score для оценки точности моделей классификации\n",
    "print('Test Accuracy : %.3f' % mlp_classifier.score(x_test, y_test))\n",
    "\n",
    "print('Training Accuracy : %.3f' % mlp_classifier.score(x_train, y_train))\n",
    "\n",
    "print('Loss : ', mlp_classifier.loss_)\n",
    "print('Number of Coefs : ', len(mlp_classifier.coefs_))\n",
    "print('Number of Intercepts : ', len(mlp_classifier.intercepts_))\n",
    "print('Number of Iterations for Which Estimator Ran : ', mlp_classifier.n_iter_)\n",
    "print('Name of Output Layer Activation Function : ', mlp_classifier.out_activation_)\n",
    "\n",
    "print('\\n**********MLPRegressor**********\\n')\n",
    "\n",
    "reg_data = pd.read_csv(urls['reg'])\n",
    "x_data, y_data = reg_data['YearsExperience'], reg_data['Salary']\n",
    "print('Dataset Sizes : ', x_data.shape, y_data.shape)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_boston, y_boston, train_size=.80, test_size=.20, random_state=123)\n",
    "print('Train/Test Sizes : ', x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp_regressor = MLPRegressor(random_state=123)\n",
    "mlp_regressor.fit(x_train, y_train)\n",
    "\n",
    "y_preds = mlp_regressor.predict(x_test)\n",
    "\n",
    "print(y_preds[:10])\n",
    "print(y_test[:10])\n",
    "# Метод Score оценивает точность моделей классификации\n",
    "print('Test R^2 Score : %.3f' % mlp_regressor.score(x_test, y_test))\n",
    "print('Training R^2 Score : %.3f' % mlp_regressor.score(x_train, y_train))\n",
    "\n",
    "print('Loss : ', mlp_regressor.loss_)\n",
    "print('Number of Coefs : ', len(mlp_regressor.coefs_))\n",
    "[weights.shape for weights in mlp_regressor.coefs_]\n",
    "print('Number of Iterations for Which Estimator Ran : ', mlp_regressor.n_iter_)\n",
    "print('Name of Output Layer Activation Function : ', mlp_regressor.out_activation_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6da2c6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}